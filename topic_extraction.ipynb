{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronpick/.virtualenvs/conversant/lib/python3.8/site-packages/benepar/spacy_plugin.py:7: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Tuple, Dict, List, Union\n",
    "import spacy\n",
    "from spacy.tokens import Span, Token\n",
    "from spacy.tokens.underscore import Underscore\n",
    "import benepar\n",
    "from benepar.integrations.spacy_extensions import ConstituentData, get_constituent\n",
    "from benepar.spacy_plugin import BeneparComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "from spacy.symbols import nsubj, nsubjpass, csubj, csubjpass, acl, agent, \\\n",
    "    dobj, iobj, nmod, attr, oprd, pobj, advcl, ccomp, acomp, prep, \\\n",
    "    xcomp, \\\n",
    "    NOUN, ADJ, VERB, PROPN\n",
    "\n",
    "SUBJ_RELATIONS = {nsubj, nsubjpass, csubj, csubjpass, acl}\n",
    "OBJ_RELATIONS = {dobj, iobj, nmod, ccomp, acomp, pobj, prep}\n",
    "\n",
    "RELEVANT_DEP_TAGS = SUBJ_RELATIONS | OBJ_RELATIONS\n",
    "\n",
    "RELEVANT_POS_TAGS = {NOUN, ADJ, VERB, PROPN}\n",
    "\n",
    "\n",
    "\n",
    "# !python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# benepar.download(\"benepar_en3\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# benepar_path = \"/Users/ronpick/workspace/zero-shot-stance/models/benepar_en3\"\n",
    "# benepar_component = BeneparComponent(\"benepar_en3\")\n",
    "# benepar_component"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "<spacy.lang.en.English at 0x1686ad430>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp = spacy.load('en')\n",
    "parser = spacy.load(\"en_core_web_sm\")\n",
    "# parser = nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
    "# parser = nlp.add_pipe(\"parser\")\n",
    "parser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# text = \"The time for action is now. It is never too late to do something.\"\n",
    "# text = \"I totally agree with this premise. As a younger person I was against Nuclear power (I was in college during 3 mile island) but now it seems that nuclear should be in the mix. Fission technology is better, and will continue to get better if we actively promote its development. The prospect of fusion energy also needs to be explored. If it's good enough for the sun and the stars, it's good enough for me.\"\n",
    "\n",
    "text1 = \"Regulation of corporations has been subverted by corporations. States that incorporate corporations are not \" \\\n",
    "       \"equipped to regulate corporations that are rich enough to influence elections, are rich enough to muster a \" \\\n",
    "       \"legal team that can bankrupt the state. Money from corporations and their principals cannot be permitted in \" \\\n",
    "       \"the political process if democracy is to survive.\"\n",
    "\n",
    "text2 = \"Absolutely it's needs to be defined and regulated in its use, as currently the word 'natural' \" \\\n",
    "       \"when used on food products is totally confusing and meaningless. Clearly they are trying to imply the item is \" \\\n",
    "       \"'healthy' or possibly 'organic', but when you see food 'manufacturers' like Frito-Lay or Campbell's with \" \\\n",
    "       \"products labelled 'natural', that alone should set off alarms that all is not what it seems. ;-)\"\n",
    "\n",
    "text3 = \"America will never be a truly great country until health care is provided for all for little to no cost. \" \\\n",
    "        \"We pay for public education whether we want to or not, we pay for wars that cost trillions. \" \\\n",
    "        \"The U.S. can afford health care for all. Just do it.\"\n",
    "\n",
    "text4 = \"While do like the 99 cent rack on my kindle book store, there's a 50/50 chance that what I look at is \" \\\n",
    "        \"self-published because before digital publishing, no one would touch it- and for good reason. A good amount \" \\\n",
    "        \"of it is really really bad. So that said, publishers offer the value added-ness, if you will, but setting a \" \\\n",
    "        \"standard that makes for an enjoyable read. I hope Amazon, if they decide to eat up the older publishers, \" \\\n",
    "        \"hires those with generations of wisdom and allows those with it to exercise it in helping me make sure that \" \\\n",
    "        \"my time spent reading is worth my while.\"\n",
    "\n",
    "text5 = \"\"\"Obesity is NOT a \"life-style choice.\" I have not eaten in a fast food restaurant in over 5 years.\n",
    "I eat no sugar at all -- no baked goods, no candy, no sodas, no jam or honey, no syrup (let alone \"a pound of double\n",
    "stuff Oreos every day\"). I eat no red meat -- just fish and lean, skinless poultry. I eat no flour, just whole grains.\n",
    "I eat fresh leafy vegetables and fruits. I eat very little dairy. I limit my calories to between 1500 and 1800 a day.\n",
    "I exercise. And yet, at 5' 7\", I weigh 215 pounds, and have for many years. Those of you who accuse me of a lack of\n",
    "\"self-discipline\" or of \"gluttony\" are ignorant bigots who should be ashamed of yourselves.\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America will never be a truly great country until health care is provided for all for little to no cost.\n",
      "['America']\n",
      "We pay for public education whether we want to or not, we pay for wars that cost trillions.\n",
      "['public education']\n",
      "The U.S. can afford health care for all.\n",
      "['U.S.', 'health care']\n",
      "Just do it.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Union\n",
    "from operator import itemgetter\n",
    "\n",
    "t: Token\n",
    "sent: Span\n",
    "def get_relevant_tokens(root: Token) -> Dict[int, Token]:\n",
    "    relevant_tokens: Dict[int, Token] = {}\n",
    "    if root.pos in RELEVANT_POS_TAGS:\n",
    "        relevant_tokens[root.i] = root\n",
    "\n",
    "    for c in root.children:\n",
    "        relevant_tokens.update(get_relevant_tokens(c))\n",
    "\n",
    "    return relevant_tokens\n",
    "\n",
    "\n",
    "def get_chunks(tokens_by_position: Dict[int, Token]) -> List[str]:\n",
    "    prev_i = -2\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for i, token in sorted(tokens_by_position.items(), key=itemgetter(0)):\n",
    "        if prev_i == i - 1:\n",
    "            if token.pos != VERB:\n",
    "                current_chunk.append(token)\n",
    "        else:\n",
    "            if len(current_chunk) > 1 or (len(current_chunk) == 1 and current_chunk[0].pos != VERB):\n",
    "                chunk_str = \" \".join([t.text for t in current_chunk])\n",
    "                chunks.append(chunk_str)\n",
    "\n",
    "            current_chunk = [token]\n",
    "\n",
    "        prev_i = i\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunk_str = \" \".join([t.text for t in current_chunk])\n",
    "        chunks.append(chunk_str)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_relevant_nps(root: Token) -> List[str]:\n",
    "    nps = []\n",
    "    xcomp_root: Union[Token, None] = None\n",
    "    extracted = False\n",
    "    for c in root.children:\n",
    "        # print(c.text)\n",
    "        if c.dep in SUBJ_RELATIONS:\n",
    "            relevant_tokens = get_relevant_tokens(c)\n",
    "            subj_chunks = get_chunks(relevant_tokens)\n",
    "            nps.extend(subj_chunks)\n",
    "            extracted = extracted or (len(subj_chunks) > 0)\n",
    "        elif c.dep in OBJ_RELATIONS:\n",
    "            relevant_tokens = get_relevant_tokens(c)\n",
    "            obj_chunks = get_chunks(relevant_tokens)\n",
    "            nps.extend(obj_chunks)\n",
    "            extracted = extracted or (len(obj_chunks) > 0)\n",
    "        elif c.dep == xcomp:\n",
    "            xcomp_root = c\n",
    "\n",
    "    if (not extracted) and (xcomp_root is not None):\n",
    "        print(\"GOING DEEPR\")\n",
    "        nps.extend(get_relevant_nps(xcomp_root))\n",
    "\n",
    "    return nps\n",
    "\n",
    "\n",
    "# find the root of the dependency parsing\n",
    "doc = parser(text3)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    # print([(t.text, t.pos_, f\"{t.head.text} -> {t.dep_}\") for t in sent])\n",
    "    nps = get_relevant_nps(sent.root)\n",
    "    print(nps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely it's needs to be defined and regulated in its use, as currently the word 'natural' when used on food products is totally confusing and meaningless.\n",
      "(Absolutely, ADV, advmod) (it, PRON, nsubj) ('s, AUX, ROOT) (needs, NOUN, attr) (to, PART, aux) (be, AUX, auxpass) (defined, VERB, xcomp) (and, CCONJ, cc) (regulated, VERB, conj) (in, ADP, prep) (its, PRON, poss) (use, NOUN, pobj) (,, PUNCT, punct) (as, ADP, mark) (currently, ADV, pcomp) (the, DET, det) (word, NOUN, dep) (', PUNCT, punct) (natural, ADJ, amod) (', PUNCT, punct) (when, ADV, advmod) (used, VERB, advcl) (on, ADP, prep) (food, NOUN, compound) (products, NOUN, pobj) (is, VERB, dep) (totally, ADV, advmod) (confusing, ADJ, acomp) (and, CCONJ, cc) (meaningless, ADJ, conj) (., PUNCT, punct)\n",
      "\t\t's-->it, nsubj\n",
      "\tit\n",
      "\t\t's-->needs, attr\n",
      "\t\tuse-->its, poss\n",
      "\t\tin-->use, pobj\n",
      "\tits use\n",
      "\t\tproducts-->food, compound\n",
      "\t\ton-->products, pobj\n",
      "\tfood products\n",
      "Clearly they are trying to imply the item is 'healthy' or possibly 'organic', but when you see food 'manufacturers' like Frito-Lay or Campbell's with products labelled 'natural', that alone should set off alarms that all is not what it seems. ;-)\n",
      "(Clearly, ADV, advmod) (they, PRON, nsubj) (are, AUX, aux) (trying, VERB, ROOT) (to, PART, aux) (imply, VERB, xcomp) (the, DET, det) (item, NOUN, nsubj) (is, AUX, ccomp) (', PUNCT, punct) (healthy, ADJ, acomp) (', PUNCT, punct) (or, CCONJ, cc) (possibly, ADV, advmod) (', PUNCT, punct) (organic, ADJ, conj) (', PUNCT, punct) (,, PUNCT, punct) (but, CCONJ, cc) (when, ADV, advmod) (you, PRON, nsubj) (see, VERB, advcl) (food, NOUN, nmod) (', PART, punct) (manufacturers, NOUN, dobj) (', PART, case) (like, ADP, prep) (Frito, PROPN, compound) (-, PUNCT, punct) (Lay, PROPN, pobj) (or, CCONJ, cc) (Campbell, PROPN, conj) ('s, PART, case) (with, ADP, prep) (products, NOUN, pobj) (labelled, VERB, acl) (', PUNCT, punct) (natural, ADJ, oprd) (', PUNCT, punct) (,, PUNCT, punct) (that, SCONJ, nsubj) (alone, ADV, advmod) (should, AUX, aux) (set, VERB, conj) (off, ADP, prt) (alarms, NOUN, dobj) (that, SCONJ, mark) (all, DET, nsubj) (is, AUX, ccomp) (not, PART, neg) (what, PRON, dobj) (it, PRON, nsubj) (seems, VERB, ccomp) (., PUNCT, punct) (;-), PUNCT, punct)\n",
      "\t\ttrying-->they, nsubj\n",
      "\tthey\n",
      "\t\titem-->the, det\n",
      "\t\tis-->item, nsubj\n",
      "\tthe item\n",
      "\t\tsee-->you, nsubj\n",
      "\tyou\n",
      "\t\tmanufacturers-->food, nmod\n",
      "\tfood 'manufacturers\n",
      "\t\tLay-->Frito, compound\n",
      "\t\tLay-->-, punct\n",
      "\t\tlike-->Lay, pobj\n",
      "\tFrito-Lay\n",
      "\t\tLay-->Campbell, conj\n",
      "\t\twith-->products, pobj\n",
      "\tproducts\n",
      "\t\tset-->alarms, dobj\n",
      "\talarms\n",
      "\t\tseems-->what, dobj\n",
      "\twhat\n",
      "\t\tseems-->it, nsubj\n",
      "\tit\n",
      "[it, its use, food products, they, the item, you, food 'manufacturers, Frito-Lay, products, alarms, what, it]\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    print(\" \".join([f\"({t.text}, {t.pos_}, {t.dep_})\" for t in sent]))\n",
    "    for chunk in sent.noun_chunks:\n",
    "        for token in chunk:\n",
    "            print(f\"\\t\\t{token.head.text}-->{token.text}, {token.dep_}\")\n",
    "            if token.dep in RELEVANT_DEP_TAGS:\n",
    "                topics.append(chunk)\n",
    "                print(f\"\\t{chunk}\")\n",
    "                # print([(c.lemma_, c.dep_) for c in chunk])\n",
    "                break\n",
    "\n",
    "print(topics)\n",
    "    #     print(\"\\t\".join(map(str, [chunk.text , chunk.lemma_, chunk.start, chunk.end])))\n",
    "    # print()\n",
    "# span: Span = chunks[0]\n",
    "# span.start, span. end, span.text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for token in sent:\n",
    "    print(token.dep_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sent._.parse_string)\n",
    "# (S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
    "print(sent._.labels)\n",
    "# ('S',)\n",
    "print(list(sent._.children))\n",
    "# The time for action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(sent.noun_chunks)\n",
    "u: Underscore = sent._\n",
    "u.span_extensions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = next(sent._.children)\n",
    "print(c._.parse_string)\n",
    "print(c._.labels)\n",
    "c = next(c._.children)\n",
    "print(c._.parse_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for (start, end, label_index) in zip(con.starts, con.ends, con.labels):\n",
    "#     label = con.label_vocab[label_index]\n",
    "#     print(start, end, label)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_from_np(np_span: Span) -> list:\n",
    "    # print(f\"NP: {np_span}\")\n",
    "    tokens = []\n",
    "    children = list(np_span._.children)\n",
    "    if len(children) == 0:\n",
    "        return [repr(np_span)]\n",
    "\n",
    "    for child in children:\n",
    "        if len(child._.labels) == 0:\n",
    "            tokens.append(repr(child))\n",
    "            # print(f\"tokens: {tokens}\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"extract: {child._.labels} - {child}\")\n",
    "        if child._.labels[0] == \"PP\":\n",
    "            # print(\"out\")\n",
    "            tokens.extend(get_NPs(child))\n",
    "            continue\n",
    "\n",
    "        # print(\"continue\")\n",
    "        tokens.extend(extract_from_np(child))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def get_NPs(span: Span) -> list:\n",
    "    # print(span)\n",
    "    nps = []\n",
    "    for child in span._.children:\n",
    "        if len(child._.labels) == 0:\n",
    "            continue\n",
    "\n",
    "        # print(f\"get: {child._.labels} - {child}\")\n",
    "        if child._.labels[0] == \"NP\":\n",
    "            nps.extend(extract_from_np(child))\n",
    "\n",
    "        nps.extend(get_NPs(child))\n",
    "\n",
    "    return nps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    nps = get_NPs(sent)\n",
    "    print(nps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}